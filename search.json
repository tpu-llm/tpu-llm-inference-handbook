[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TPU LLM Inference Handbook",
    "section": "",
    "text": "Part 1: The foundations\nLet’s start our journey with a simple, hands-on experiment. Before we dive into the complexities of the Large Language Model (LLM) and specialized accelerator hardware, let’s run a single piece of Python code. This script performs one of the most fundamental operations in computing: multiplying two matrices together.\nReview the following python program (also available in source repo).\nThis simple calculation, a batch of computational “heartbeats” is the key to understanding the immense power and challenge of running modern artificial intelligence.\nRun this program locally (You will need to have python installed and configured).\nObserve how long it takes to complete the task. It might take an average of ~30 - 50ms milliseconds per iteration on a modern laptop.\nThis guide is the first in a series dedicated to harnessing the power of Google’s Tensor Processing Units (TPUs) for inference. We will build our understanding from the ground up, moving from architectural theory to practical, hands-on application.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part 1: the foundation</span>"
    ]
  },
  {
    "objectID": "index.html#part-1-the-foundations",
    "href": "index.html#part-1-the-foundations",
    "title": "TPU LLM Inference Handbook",
    "section": "",
    "text": "Click to see code\n\nimport torch\nimport time\n\n# --- Benchmark Configuration ---\n# BENCHMARK_ITERATIONS mimic the steady-state operation of \n# an inference server under a continuous load of user requests.\nBENCHMARK_ITERATIONS = 100\n\ndef run_matmul_on_cpu():\n    # Set the device explicitly to CPU\n    device = torch.device(\"cpu\")\n    print(f\"Using device: {device}\")\n\n    # Define matrix dimensions (conceptual for an LLM layer)\n    # batch_size: number of input sequences processed in parallel\n    # hidden_size: dimension of the hidden states (e.g., 4096, 8192)\n    # intermediate_size: typically 4x hidden_size in Transformer FFNs\n    batch_size = 128\n    hidden_size = 4096\n    intermediate_size = 16384 # 4 * hidden_size for FFN\n\n    print(f\"Simulating MatMul for LLM Feed-Forward Network:\")\n    print(f\"  Batch size: {batch_size}\")\n    print(f\"  Input features (hidden_size): {hidden_size}\")\n    print(f\"  Output features (intermediate_size): {intermediate_size}\")\n\n    # Create random input tensor and weight matrix on the CPU device\n    # Using float32 (standard float) for CPU computation\n    input_tensor = torch.randn(batch_size, hidden_size, dtype=torch.float32).to(device)\n    weight_matrix = torch.randn(hidden_size, intermediate_size, dtype=torch.float32).to(device)\n\n    print(f\"\\nInput tensor shape: {input_tensor.shape} (dtype: {input_tensor.dtype})\")\n    print(f\"Weight matrix shape: {weight_matrix.shape} (dtype: {weight_matrix.dtype})\")\n\n    # --- Benchmark Phase ---\n    print(f\"Performing {BENCHMARK_ITERATIONS} benchmark iterations...\")\n    start_time = time.perf_counter()\n    for _ in range(BENCHMARK_ITERATIONS):\n        output_tensor = torch.matmul(input_tensor, weight_matrix)\n    end_time = time.perf_counter()\n    total_time_ms = (end_time - start_time) * 1000\n\n    print(f\"\\n--- Results ---\")\n    print(f\"Total time for {BENCHMARK_ITERATIONS} iterations: {total_time_ms:.4f} ms\")\n    print(f\"Average time per iteration: {total_time_ms / BENCHMARK_ITERATIONS:.4f} ms\")\n\n    # Clean up (optional)\n    del input_tensor, weight_matrix, output_tensor\n\nif __name__ == \"__main__\":\n    try:\n        print(\"--- Matrix Multiplication Benchmark (CPU) ---\")\n        run_matmul_on_cpu()\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        print(\"Please ensure you have PyTorch installed to run this code.\")\n\n\n\npip install torch\npython matrix_heartbeat_cpu.py\n\n\n\nPart 1 (This Post): The journey begins with the fundamentals. We will explore the TPU inference architecture, set up a Google Cloud environment, provision our first TPU Virtual Machine (VM), and re-run our “heartbeat” script to witness a dramatic acceleration.\n\nPart 2 & Beyond: Building on this foundation, the series will progress to deploying a full LLM (like Meta’s Llama 3), exploring advanced serving techniques with high-performance systems, and ultimately scaling our solution for production using Google Kubernetes Engine (GKE).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part 1: the foundation</span>"
    ]
  },
  {
    "objectID": "index.html#from-simple-math-to-complex-ai-the-transformers-core",
    "href": "index.html#from-simple-math-to-complex-ai-the-transformers-core",
    "title": "TPU LLM Inference Handbook",
    "section": "From Simple Math to Complex AI: The Transformer’s Core",
    "text": "From Simple Math to Complex AI: The Transformer’s Core\nThe script you just ran performed a single matrix multiplication. It may seem basic, but this exact mathematical operation, scaled up thousands of times and chained together in intricate layers, is the fundamental building block of the Transformer architecture. This architecture is the engine behind virtually all state-of-the-art LLMs.\nWhen an LLM generates text, it’s not “thinking” in a human sense. It is, at its core, executing a staggering number of matrix and vector computations. These operations occur in two key parts of every Transformer layer:\n\nSelf-Attention: This is the mechanism that allows the model to weigh the importance of different words in the input text. To do this, it transforms the input data into three matrices: Query (Q), Key (K), and Value (V). The core of the attention calculation is a massive matrix multiplication of Q and K.\n\nFeed-Forward Networks: After the attention mechanism, the data passes through a standard neural network layer, which again is fundamentally a series of matrix multiplications followed by non-linear activation functions.\n\nAn LLM with billions of parameters is, in essence, a vast collection of these matrix operations. Running inference—the process of generating a response from a trained model—requires executing these calculations as quickly and efficiently as possible. Your CPU can handle one multiplication, but to serve a real-world application, we need hardware specifically designed for this task.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part 1: the foundation</span>"
    ]
  },
  {
    "objectID": "index.html#the-rhythmic-heart-of-ai-why-tpus-excel-at-llm-inference",
    "href": "index.html#the-rhythmic-heart-of-ai-why-tpus-excel-at-llm-inference",
    "title": "TPU LLM Inference Handbook",
    "section": "The Rhythmic Heart of AI: Why TPUs Excel at LLM Inference",
    "text": "The Rhythmic Heart of AI: Why TPUs Excel at LLM Inference\nTo truly appreciate why a developer should invest time in learning TPUs, it is essential to move beyond the simple statement “it’s fast” and delve into the architectural elegance that makes this speed possible. The advantage of the TPU is not just an incremental improvement; it is a paradigm shift in processing designed specifically for the mathematics of machine learning.\nTraditional processors, including Central Processing Units (CPUs) and Graphics Processing Units (GPUs), are built upon the von Neumann architecture. This model involves a processing unit that fetches instructions and data from a separate memory unit, performs a computation, and writes the result back to memory. An effective analogy is a chef (the processor) who must constantly run to a pantry (memory) to retrieve one ingredient at a time for each step of a recipe. This constant back-and-forth travel creates a significant performance limitation known as the “von Neumann bottleneck”. While GPUs dramatically increase performance by employing thousands of chefs working in parallel, they all still run to the same pantry, meaning memory access remains a fundamental constraint.\nThe Tensor Processing Unit (TPU) was designed to solve this very problem for machine learning workloads. At its core is a component called the Matrix Multiply Unit (MXU), which contains a systolic array. This architecture represents a complete departure from the fetch-and-execute model. The term “systolic” is an analogy to the human circulatory system, where the heart pumps blood in a steady, rhythmic pulse. Similarly, in a TPU, data is not fetched from memory for each individual calculation. Instead, it is pumped rhythmically through a large, two-dimensional grid of simple processors.\nEach cell in the systolic array performs a simple multiply-accumulate (MAC) operation on the data it receives from an upstream neighbor, and then passes the partial result to its downstream neighbor in perfect time with a system clock. Weights are pre-loaded into the array, and input data flows through it in a wave. The final results of a massive matrix multiplication emerge from the array over time, with minimal need for the processors to access main memory during the computation itself. This “data-in-motion” paradigm effectively designs the memory bottleneck out of the most intensive part of the neural network calculation.\nUltimately, these architectural advantages translate into a critical business metric: price-performance. For large-scale inference, faster computation means fewer accelerators are needed to serve the same number of users, which directly lowers operational costs. For any large and complex LLM, this efficiency is not a minor optimization; it is what makes serving it economically viable at scale. This proven cost-efficiency, rooted in the TPU’s long history of powering massive internal Google services like Search, Photos, and Translate, establishes it as a premier platform for deploying the next generation of machine learning models.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part 1: the foundation</span>"
    ]
  },
  {
    "objectID": "index.html#tpu-inference-stack",
    "href": "index.html#tpu-inference-stack",
    "title": "TPU LLM Inference Handbook",
    "section": "TPU inference stack",
    "text": "TPU inference stack\nRunning large language models efficiently requires a full “stack” of technology where each layer is optimized to work with the one above it, as shown in the diagram below.\n\n\n\n\n\nThe foundation is the TPU designed specifically for the massive matrix calculations at the heart of AI. This powerful compute is connected by ultra-fast networking and paired with a flexible storage system. On top of this hardware runs the open software layer. This includes essential tools like Google Kubernetes Engine (GKE) and Compute Engine to manage the infrastructure, popular ML frameworks like JAX and PyTorch to build models, and the critical XLA compiler that translates framework code into highly efficient instructions for the TPUs.\nThis integrated platform hosts the Large Language Models, from Google’s Gemma to other models like Llama and DeepSeek. The final piece of the stack is the Inference Engine, such as JetStream or vLLm. This top-level software acts as a smart conductor, taking a trained model and serving it for real-world applications. It handles incoming user requests, manages memory, and optimizes the model’s execution to ensure maximum speed and throughput. Every layer in this stack works in harmony to make demanding AI inference fast, scalable, and efficient.\nFor the purposes of this introductory guide, we will use the Cloud TPU VM (Compute Engine) to get a taste of TPU.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part 1: the foundation</span>"
    ]
  },
  {
    "objectID": "index.html#setup-the-tpu-inference-infrastructure",
    "href": "index.html#setup-the-tpu-inference-infrastructure",
    "title": "TPU LLM Inference Handbook",
    "section": "Setup the TPU inference infrastructure",
    "text": "Setup the TPU inference infrastructure\nThis section provides a guide to setting up the necessary accounts, permissions, tools, and TPU.\nYou will need a Google Cloud Project with billing enabled. Please follow the official Google Cloud documentation to prepare your Google Cloud environment. Make sure that the following tasks are completed:\nGoogle Cloud Project Foundation:\n\nCreate or pick a Google Cloud Project.\n\nEnable billing for that project.\n\nInstall and configure the gcloud CLI.\n\nActivate the tpu.googleapis.com API.\n\nPermissions & Service Accounts:\n\nEnsure your user account has Service Account Admin, Project IAM Admin, and TPU Admin roles.\n\nCreate a Cloud TPU Service Agent.\n\nCreate a dedicated, user-managed TPU Service Account with appropriate roles (e.g., TPU Admin, Storage Admin).\n\nQuota Management (Crucial!):\n\nRequest quota increases for your specific TPU cores (e.g., v6e), IP addresses, Hyperdisk Balanced Capacity, and CPUs for the host VM. Plan for a 1-2 day approval time.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part 1: the foundation</span>"
    ]
  },
  {
    "objectID": "index.html#the-tpu-performance-taste",
    "href": "index.html#the-tpu-performance-taste",
    "title": "TPU LLM Inference Handbook",
    "section": "The TPU Performance Taste",
    "text": "The TPU Performance Taste\nWith all the preparation complete, it is time to provision an TPU VM and runn our script again to see the performance difference firsthand.\nYou can provision TPU VM with automation tools like Terraform, Cloud APIs, Console UI or command line tool gcloud, which is the simplest approach for the handbook.\nThe following command creates a TPU Trillium v6e with 1 core (smallest TPU unit). Note: Set the ZONE to an GCP zone with proper TPU quota configured.\n# Set environment variables for clarity  \nexport PROJECT_ID=your-project-id  \nexport TPU_NAME=llm-tpu-vm  \nexport ZONE=us-central1-b  \nexport ACCELERATOR_TYPE=v6e-1  \nexport RUNTIME_VERSION=v2-alpha-tpuv6e\n\n# The command to create the TPU VM  \ngcloud compute tpus tpu-vm create $TPU_NAME \\  \n  –project=$PROJECT_ID \\  \n  --zone=$ZONE \\  \n  --accelerator-type=$ACCELERATOR_TYPE \\  \n  --version=$RUNTIME_VERSION \nOnce TPU VM is provisioned, you can validate the status by running command:\ngcloud compute tpus tpu-vm list --zone=$ZONE  \nLook for the STATUS column for your TPU VM’s name, and make sure it is in READY state\nNow you can connect to it and install the necessary Python libraries.\n\nSSH into the VM: Use the gcloud command to establish a secure shell connection.\n\ngcloud compute tpus tpu-vm ssh $TPU_NAME \\  \n    --project=$PROJECT_ID \\  \n    --zone=$ZONE\nThis command places the user in a terminal on the TPU host machine.\nInstall PyTorch/XLA: Inside the VM, install the PyTorch libraries that are specifically compiled to work with TPUs via the XLA compiler. Using the -f flag to point to the Google Storage release index.\n\n\nsudo apt-get update && sudo apt-get install libopenblas-dev -y   \npip install numpy \npip install torch torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html\nNow, we will run a slightly modified version of our original script. This version adds the necessary torch_xla code to ensure the computation runs on the TPU hardware.\nThe original script is modified to use TPU to run the matrix calculation:\n\n\nClick to see code\n\nimport torch\nimport time\nimport torch_xla\nimport torch_xla.core.xla_model as xm\n\n# --- Benchmark Configuration ---\nBENCHMARK_ITERATIONS = 100\n\ndef run_matmul_on_tpu():\n    # Set the device to TPU\n    device = xm.xla_device()\n    print(f\"Using device: {device}\")\n\n    # Define matrix dimensions (conceptual for an LLM layer)\n    # batch_size: number of input sequences processed in parallel\n    # hidden_size: dimension of the hidden states (e.g., 4096, 8192)\n    # intermediate_size: typically 4x hidden_size in Transformer FFNs\n    batch_size = 128 # Set to match CPU for fair comparison. Can be lowered if OOM.\n    hidden_size = 4096\n    intermediate_size = 16384 # 4 * hidden_size for FFN\n\n    print(f\"Simulating MatMul for LLM Feed-Forward Network:\")\n    print(f\"  Batch size: {batch_size}\")\n    print(f\"  Hidden size: {hidden_size}\")\n    print(f\"  Intermediate size: {intermediate_size}\")\n\n    # Create random input tensor and weight matrix on the TPU device\n    input_tensor = torch.randn(batch_size, hidden_size, dtype=torch.float32).to(device)\n    weight_matrix = torch.randn(hidden_size, intermediate_size, dtype=torch.float32).to(device)\n\n    print(f\"\\nInput tensor shape: {input_tensor.shape} (dtype: {input_tensor.dtype})\")\n    print(f\"Weight matrix shape: {weight_matrix.shape} (dtype: {weight_matrix.dtype})\")\n\n    # --- Benchmark Phase ---\n    print(f\"Performing {BENCHMARK_ITERATIONS} benchmark iterations...\")\n    start_time = time.perf_counter()\n    for _ in range(BENCHMARK_ITERATIONS):\n        output_tensor = torch.matmul(input_tensor, weight_matrix)\n    xm.mark_step() # Ensure all benchmark operations are complete before stopping timer\n    end_time = time.perf_counter()\n    total_time_ms = (end_time - start_time) * 1000\n\n    print(f\"\\n--- Results ---\")\n    print(f\"Total time for {BENCHMARK_ITERATIONS} iterations: {total_time_ms:.4f} ms\")\n    print(f\"Average time per iteration: {total_time_ms / BENCHMARK_ITERATIONS:.4f} ms\")\n    # Move output tensor to CPU for further processing/inspection if needed\n    output_tensor = output_tensor.cpu()\n\n    # Clean up (optional)\n    del input_tensor, weight_matrix, output_tensor\n\nif __name__ == \"__main__\":\n    try:\n        print(\"--- Matrix Multiplication Benchmark (TPU) ---\")\n        run_matmul_on_tpu()\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        print(\"Please ensure you have PyTorch and Torch_XLA installed to run this code and you are running in a TPU environment.\")\n\nRun the script (or use the sample code from repo) from the TPU VM’s terminal:\npython matrix_heartbeat_tpu.py\nThe script output should look similar to this:\n--- Matrix Multiplication Benchmark (TPU) ---\nUsing device: xla:0\nSimulating MatMul for LLM Feed-Forward Network:\n  Batch size: 128\n  Hidden size: 4096\n  Intermediate size: 16384\n\nInput tensor shape: torch.Size([128, 4096]) (dtype: torch.float32)\nWeight matrix shape: torch.Size([4096, 16384]) (dtype: torch.float32)\nPerforming 100 benchmark iterations...\n\n--- Results ---\nTotal time for 100 iterations: 383.0780 ms\nAverage time per iteration: 3.8308 ms\nThe key indicator of success is seeing the device listed as xla:0 (or another number). This confirms that the tensors were created on the TPU and the computation was executed there.\nHowever, you may notice that average time per iteration is probably just several times faster than running on CPU, what happened?!\nThe reason is due to two main factors: JIT Compilation Overhead, the XLA (Accelerated Linear Algebra) compiler performs a one-time, Just-In-Time (JIT) compilation and Data Transfer, the tensors first have to be moved from the host CPU’s memory over the PCIe bus to the TPU’s own High-Bandwidth Memory (HBM). TPUs are designed for sustained, high-throughput workloads, not single, short tasks. The real power becomes evident when you run the same compiled graph many times. The initial compilation cost is paid only once, and then every subsequent execution is incredibly fast. Let’s add the following warm-up iteration to the TPU program (add it right before the # — Benchmark Phase — ):\n    # --- Warm-up Phase (triggers XLA compilation) ---\n    WARMUP_ITERATIONS = 5\n    print(f\"\\nPerforming {WARMUP_ITERATIONS} warm-up iterations...\")\n    for _ in range(WARMUP_ITERATIONS):\n        _ = torch.matmul(input_tensor, weight_matrix)\n    xm.mark_step() # Wait for all warm-up iterations to complete\nNow, compare the TPU execution time to the time you recorded from your local CPU. You should see a dramatic difference — what took 30ms on a general-purpose CPU is completed in a fraction of a millisecond on hardware purpose-built for this exact task. You have just witnessed the power of accelerated computing.\nResult from v6e-1 for 100 batch:\n--- Results ---\nTotal time for 100 iterations: 0.5619 ms\nAverage time per iteration: 0.0056 ms",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part 1: the foundation</span>"
    ]
  },
  {
    "objectID": "index.html#conclusion-the-first-pulse",
    "href": "index.html#conclusion-the-first-pulse",
    "title": "TPU LLM Inference Handbook",
    "section": "Conclusion: The First Pulse",
    "text": "Conclusion: The First Pulse\nThis first stage of our journey has laid a critical foundation. We started with a simple matrix multiplication, and used it as a lens to understand the computational core of modern AI. We have navigated the practical Gogole Cloud setup, and successfully provisioned a TPU VM and established communication with it.\nMost importantly, by running our “heartbeat” script on both a CPU and a TPU, we have a tangible, dramatic demonstration of the performance gains offered by specialized accelerators. The foundation is now firmly in place. In Part 2 of this series, we will build upon this knowledge to load and run a full-scale Large Language Model, turning this raw computational power into intelligent text generation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Part 1: the foundation</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  }
]